---
# ========= 0) Dossiers de travail (compose + jmx) =========
# Création des répertoires nécessaires au fonctionnement de Docker Compose et JMX.
# Ces dossiers contiendront la configuration, les agents et autres fichiers utilisés par Kafka.
- name: Ensure compose working dirs exist
  file:
    path: "{{ item }}"
    state: directory
    mode: "0755"
  loop:
    - "{{ compose_workdir }}"
    - "{{ compose_workdir }}/config"
    - "{{ compose_workdir }}/jmx"

# ========= 1) Data / Logs : droits pour appuser (UID/GID 1000) =========
# Kafka utilise l’utilisateur 1000 dans le conteneur (appuser). On s’assure que les répertoires
# de données et de logs existent et qu’ils appartiennent à cet utilisateur.
- name: Ensure data/logs dirs exist (owned by container UID/GID)
  file:
    path: "{{ item }}"
    state: directory
    owner: "{{ container_uid | default(1000) }}"
    group: "{{ container_gid | default(1000) }}"
    mode: "0755"
  loop: "{{ kafka_data_dirs + [kafka_log_dir] }}"

# Vérifie récursivement la propriété et les permissions sur les dossiers data/logs
# (sans provoquer d’erreur ni changement s’ils existent déjà).
- name: Ensure recursive ownership on data/logs (idempotent)
  file:
    path: "{{ item }}"
    state: directory
    recurse: true
    owner: "{{ container_uid | default(1000) }}"
    group: "{{ container_gid | default(1000) }}"
  loop: "{{ kafka_data_dirs + [kafka_log_dir] }}"
  changed_when: false
  failed_when: false

# ========= 2) JMX (Kafka) =========
# Télécharge l’agent JMX Prometheus pour exposer les métriques Kafka à Prometheus.
- name: Download jmx_prometheus_javaagent.jar
  get_url:
    url: "https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.20.0/jmx_prometheus_javaagent-0.20.0.jar"
    dest: "{{ compose_workdir }}/jmx/jmx_prometheus_javaagent.jar"
    mode: "0777"
  when: jmx_enabled | default(true)

# Copie la configuration JMX personnalisée pour Kafka.
- name: Copy kafka_jmx.yml
  copy:
    src: "kafka_jmx.yml"
    dest: "{{ compose_workdir }}/jmx/kafka.yml"
    mode: "0644"
  when: jmx_enabled | default(true)

- name: Render kafka_server_jaas.conf
  template:
    src: kafka_server_jaas.conf.j2
    dest: "{{ compose_workdir }}/config/kafka_server_jaas.conf"
    mode: "0644"
  notify: Restart kafka compose

- name: Copy Kafka truststore
  copy:
    src: "truststore.jks"
    dest: "{{ compose_workdir }}/config/truststore.jks"
    mode: "0777"
  notify: Restart kafka compose

- name: Copy Kafka keystore
  copy:
    src: "keystore.jks"
    dest: "{{ compose_workdir }}/config/keystore.jks"
    mode: "0777"
  notify: Restart kafka compose


- name: Create SSL credentials files for Kafka
  copy:
    dest: "{{ compose_workdir }}/config/{{ item.filename }}"
    content: "{{ item.content }}\n"
    owner:  "{{ container_uid | default(1000) }}"
    group:  "{{ container_uid | default(1000) }}"
    mode: '777'
  loop:
    - { filename: "keystore.creds",   content: "{{ kafka_ssl_keystore_password }}" }
    - { filename: "key.creds",        content: "{{ kafka_ssl_key_password | default(kafka_ssl_keystore_password) }}" }
    - { filename: "truststore.creds", content: "{{ kafka_ssl_truststore_password }}" }
  notify: Restart kafka compose

# ========= 3) server.properties =========

- name: Render server.properties (bootstrap / PLAINTEXT)
  template:
    src: "server.bootstrap.properties.j2"
    dest: "{{ compose_workdir }}/config/server.properties"
    mode: "0644"
    backup: yes
  notify: Restart kafka compose
  when: not sasl_enabled

- name: Render server.properties (SASL_SSL + SCRAM)
  template:
    src: "server.secure.properties.j2"
    dest: "{{ compose_workdir }}/config/server.properties"
  notify: Restart kafka compose
  when: sasl_enabled

- name: Render client.properties
  template:
    src: "client.properties.j2"
    dest: "{{ compose_workdir }}/config/client.properties"
  notify: Restart kafka compose
  when: sasl_enabled 

# ========= 4) Détection format KRaft & format si nécessaire =========
# Cette section détecte si le cluster Kafka en mode KRaft est déjà initialisé.
# Si ce n’est pas le cas, ou si un reformatage forcé est demandé, elle formate le stockage.

# 4.1 Vérifie la présence du fichier meta.properties (preuve de formatage KRaft existant)
- name: Check for meta.properties existence
  stat:
    path: "{{ kafka_data_dirs[0] }}/meta.properties"
  register: meta_stat

# 4.2 Lit le fichier meta.properties si présent (encodé en base64)
- name: Read meta.properties content when present
  slurp:
    path: "{{ kafka_data_dirs[0] }}/meta.properties"
  register: meta_slurp
  when: meta_stat.stat.exists

# 4.3 Extrait le cluster.id depuis le fichier meta.properties
- name: Extract detected cluster.id
  set_fact:
    detected_cluster_id: >-
      {{
        (meta_slurp.content
         | b64decode
         | regex_findall('(?mi)^cluster\\.id\\s*=\\s*(\\S+)')
         | first)
        | default('', true)
      }}
  when: meta_stat.stat.exists

# 4.4 Détermine les actions à entreprendre :
# - format_needed : si meta.properties absent ou cluster.id vide → format initial requis.
# - reformat_conflict : si cluster.id différent du kafka_cluster_id attendu → conflit.
# - kafka_force_reformat : permet de forcer un reformatage destructif (false par défaut).
- name: Decide whether format is needed
  set_fact:
    format_needed: "{{ (not meta_stat.stat.exists) or ((detected_cluster_id | default('')) | length == 0) }}"
    reformat_conflict: >-
      {{
        meta_stat.stat.exists
        and ((detected_cluster_id | default('')) | length > 0)
        and (detected_cluster_id != kafka_cluster_id)
      }}
    kafka_force_reformat: "{{ kafka_force_reformat | default(false) }}"

# 4.5 Arrête l’exécution si un conflit d’ID est détecté et qu’un reformatage forcé n’est pas autorisé.
- name: Fail on cluster.id mismatch unless kafka_force_reformat=true
  fail:
    msg: >-
      Data dir {{ kafka_data_dirs[0] }} déjà formaté avec cluster.id={{ detected_cluster_id }},
      qui ne correspond pas à kafka_cluster_id={{ kafka_cluster_id }}.
      Fixez kafka_force_reformat=true pour purger et reformater (DESTRUCTIF), ou alignez les IDs.
  when: reformat_conflict and not kafka_force_reformat

# 4.6 Si le reformatage est forcé, supprime complètement le répertoire de données Kafka
# puis le recrée vide avec les bons droits.
- name: Purge data dir before forced reformat (DESTRUCTIF)
  file:
    path: "{{ kafka_data_dirs[0] }}"
    state: absent
  when: (reformat_conflict and kafka_force_reformat) and (not ansible_check_mode)

- name: Recreate data dir (after forced purge or initial)
  file:
    path: "{{ kafka_data_dirs[0] }}"
    state: directory
    owner: "{{ container_uid | default(1000) }}"
    group: "{{ container_gid | default(1000) }}"
    mode: "0755"
  when: ((reformat_conflict and kafka_force_reformat) or format_needed) and (not ansible_check_mode)

# 4.7 Génère un fichier de configuration minimal (server.format.properties)
# utilisé uniquement pour exécuter la commande kafka-storage format.
- name: Render server.format.properties (minimal for format)
  template:
    src: "server.format.properties.j2"
    dest: "{{ compose_workdir }}/config/server.format.properties"
    mode: "0644"
    backup: yes
  when: ((reformat_conflict and kafka_force_reformat) or format_needed) and (not ansible_check_mode)

# 4.8 Formate le stockage Kafka KRaft via Docker (commande kafka-storage format).
# Cette étape initialise ou réinitialise le cluster.id selon les conditions suivantes :
#   - format_needed = true (aucun meta.properties trouvé), ou
#   - reformat_conflict + kafka_force_reformat = true (reformatage forcé).
# Le paramètre "creates" empêche de reformater si le fichier meta.properties existe déjà.
# En cas d’erreur "already formatted", Ansible considère la tâche comme réussie.
- name: Format KRaft storage (fixed cluster id) via docker CLI
  command: >
    docker run --rm
    -v {{ compose_workdir }}/config/server.format.properties:/etc/kafka/server.properties:ro
    -v {{ kafka_data_dirs[0] }}:{{ kafka_data_dirs[0] }}
    --entrypoint bash
    confluentinc/cp-kafka:{{ confluent_version }}
    -lc "set -e; kafka-storage format -t \"{{ kafka_cluster_id }}\" -c /etc/kafka/server.properties"
  args:
    creates: "{{ kafka_data_dirs[0] }}/meta.properties"
  register: format_result
  changed_when: format_result.rc == 0
  failed_when: >
    format_result.rc != 0 and
    ('already formatted' not in (format_result.stderr | default('') | lower))
  when: ((reformat_conflict and kafka_force_reformat) or format_needed) and (not ansible_check_mode)

# 4.9 Vérifie et affiche le résultat du formatage :
#   - Relit meta.properties pour s’assurer que le format a bien été effectué.
#   - Extrait le cluster.id généré ou confirmé.
#   - Affiche la valeur finale pour validation visuelle.
- name: Re-read meta.properties after (potential) format
  slurp:
    path: "{{ kafka_data_dirs[0] }}/meta.properties"
  register: meta_slurp_after
  when: (reformat_conflict and kafka_force_reformat) or format_needed or meta_stat.stat.exists

# Extrait le cluster.id actuel après formatage ou relecture.
# Cela permet de confirmer le bon identifiant du cluster sur ce nœud.
- name: Extract cluster.id after (potential) format
  set_fact:
    detected_cluster_id_final: >-
      {{
        (meta_slurp_after.content
         | b64decode
         | regex_findall('(?mi)^cluster\\.id\\s*=\\s*(\\S+)')
         | first)
        | default('N/A', true)
      }}
  when: (reformat_conflict and kafka_force_reformat) or format_needed or meta_stat.stat.exists

# Affiche dans la sortie Ansible le cluster.id final du nœud.
# Cela permet de valider que le cluster Kafka est correctement initialisé.
- name: Show detected cluster.id
  debug:
    msg: "Node {{ inventory_hostname }}: cluster.id={{ detected_cluster_id_final }}"

# ========= 5) docker-compose.yml + déploiement (sans recréer) =========
# Génère le fichier docker-compose.yml utilisé pour déployer Kafka via Docker Compose.
- name: Render docker-compose.yml
  template:
    src: "docker-compose.kafka.yml.j2"
    dest: "{{ compose_workdir }}/docker-compose.yml"
    mode: "0644"
    backup: yes
  notify: Recreate Container

# Détecte la version de Docker Compose disponible (v2 prioritaire, sinon v1 classique).
- name: Detect compose command (v2 preferred, else v1)
  shell: |
    set -e
    if docker compose version >/dev/null 2>&1; then
      echo "docker compose"
    elif docker-compose --version >/dev/null 2>&1; then
      echo "docker-compose"
    else
      echo ""
    fi
  register: compose_cmd_probe
  changed_when: false

# Définit la variable compose_cmd avec la commande trouvée.
- name: Set compose_cmd fact
  set_fact:
    compose_cmd: "{{ compose_cmd_probe.stdout | trim }}"

# Stoppe le playbook si aucune commande Docker Compose n’est détectée.
- name: Fail if no compose command found
  fail:
    msg: "Neither 'docker compose' nor 'docker-compose' is available on this host."
  when: compose_cmd | length == 0

# Télécharge les images Docker Kafka depuis le registre.
- name: Pull Kafka images
  command: "{{ compose_cmd }} -f docker-compose.yml pull"
  args:
    chdir: "{{ compose_workdir }}"
  register: _pull_out
  changed_when: >
    ('Downloaded' in (_pull_out.stdout | default(''))) or
    ('Pulling' in (_pull_out.stdout | default(''))) or
    ('Downloading' in (_pull_out.stdout | default('')))

# Démarre Kafka via Docker Compose sans recréer les conteneurs existants.
- name: Up Kafka via Compose (no recreate, keep existing)
  command: "{{ compose_cmd }} -f docker-compose.yml up -d --no-recreate"
  args:
    chdir: "{{ compose_workdir }}"
  register: _up_out
  changed_when: >
    ('Creating' in (_up_out.stdout | default(''))) or
    ('Recreating' in (_up_out.stdout | default(''))) or
    ('Starting' in (_up_out.stdout | default('')))

# ========= 6) Health check ports =========
# Attend que les ports du contrôleur et du broker Kafka soient accessibles,
# garantissant que Kafka est bien démarré et opérationnel.
- name: Wait for controller port
  wait_for:
    host: "127.0.0.1"
    port: "{{ kafka_controller_port }}"
    timeout: 60

- name: Wait for broker port
  wait_for:
    host: "127.0.0.1"
    port: "{{ kafka_broker_port }}"
    timeout: 60

# ========= 7) Création / mise à jour du user admin SCRAM =========

- name: Create or update Kafka SCRAM admin user
  command: >
    docker exec
    -e KAFKA_OPTS=""
    -e KAFKA_JMX_OPTS=""
    -e JMX_PORT=""
    kafka-{{ inventory_hostname_short | default(inventory_hostname) }}
    kafka-configs
    --bootstrap-server localhost:{{ kafka_broker_port }}
    --alter
    --add-config SCRAM-SHA-256=[iterations={{ kafka_admin_iterations }},password={{ kafka_scram_password }}]
    --entity-type users
    --entity-name {{ kafka_scram_user }}
  when:
    - inventory_hostname == groups['kafka_brokers'][0]
    - not sasl_enabled
  changed_when: false

- name: Verify Kafka SCRAM user exists
  command: >
    docker exec
    -e KAFKA_OPTS=""
    -e KAFKA_JMX_OPTS=""
    -e JMX_PORT=""
    kafka-{{ inventory_hostname_short | default(inventory_hostname) }}
    kafka-configs
    --bootstrap-server localhost:{{ kafka_broker_port }}
    --describe
    --entity-type users
    --entity-name {{ kafka_scram_user }}
  register: kafka_user_check
  when:
    - inventory_hostname == groups['kafka_brokers'][0]
    - not sasl_enabled
  changed_when: false

- name: Show SCRAM user verification result
  debug:
    msg: "{{ kafka_user_check.stdout }}"
  when:
    - inventory_hostname == groups['kafka_brokers'][0]
    - kafka_user_check is defined
    - not sasl_enabled
